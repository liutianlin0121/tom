{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute all empirical results for the PhD thesis\n",
    "\n",
    "This requires:\n",
    "* the `tom` toolkit to be compiled and installed\n",
    "* Python 3, numpy, scipy, ipython, ipyparallel, jupyter\n",
    "* A running ipyparallel cluster with 3 engines (from this directory?)\n",
    "* the `Tools.ipynb` script and benchmarks\n",
    "* 8GB memory + 8GB fast swap, or 16GB memory\n",
    "* 5GB free disk space\n",
    "* Approx. 2.5 days computation time on my MacBok (2 GHz Intel Core i7 (4 cores)):\n",
    "    - ~ 12 hours for the comparison results\n",
    "    - ~ 52 hours for the missing values results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize parallel computation (require 3 clients)\n",
    "research_directory = %pwd\n",
    "tools_script = research_directory + '/Tools.ipynb'\n",
    "%run $tools_script\n",
    "from ipyparallel import Client\n",
    "rc = Client(); dview = rc[:]\n",
    "if len(dview) != 3: raise RuntimeError('Want to have 3 parallel clients.')\n",
    "for i in range(3): rc[i]['ID'] = i\n",
    "ID = 0\n",
    "dview['research_directory'] = research_directory\n",
    "%px %run $tools_script\n",
    "%px tom.util.mkl_set_num_threads(4)\n",
    "%px tom.util.setNbThreads(4)\n",
    "%px tom.util.omp_set_num_threads(2)\n",
    "%px exec(\"try: import mkl; mkl.set_num_threads(2)\\nexcept: pass\")\n",
    "assert(dview['tom.version'] == [tom.version]*3)\n",
    "tom.util.mkl_set_num_threads(8)\n",
    "tom.util.setNbThreads(8)\n",
    "tom.util.omp_set_num_threads(8)\n",
    "display_width(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Global settings:\n",
    "%px regularization = (2,3)\n",
    "regularization = (2,3)\n",
    "import os\n",
    "os.makedirs(research_directory + '/results', exist_ok=True)\n",
    "os.makedirs(research_directory + '/results/RANDOM27_32_max_data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RANDOM4_7: Σ= 4 L=5  Σ^L= 1024 Σ^(L+1)= 4096 Σ^{<L}= 1365\n",
      "  RANDOM4_32: Σ= 4 L=5  Σ^L= 1024 Σ^(L+1)= 4096 Σ^{<L}= 1365\n",
      "  RANDOM27_7: Σ=27 L=2  Σ^L=  729 Σ^(L+1)=19683 Σ^{<L}=  757\n",
      " RANDOM27_32: Σ=27 L=2  Σ^L=  729 Σ^(L+1)=19683 Σ^{<L}=  757\n",
      "       TIGER: Σ= 6 L=3  Σ^L=  216 Σ^(L+1)= 1296 Σ^{<L}=  259\n",
      "       PAINT: Σ= 8 L=3  Σ^L=  512 Σ^(L+1)= 4096 Σ^{<L}=  585\n",
      "      BRIDGE: Σ=60 L=1  Σ^L=   60 Σ^(L+1)= 3600 Σ^{<L}=   61\n",
      "     NETWORK: Σ= 8 L=3  Σ^L=  512 Σ^(L+1)= 4096 Σ^{<L}=  585\n",
      "     SHUTTLE: Σ=15 L=2  Σ^L=  225 Σ^(L+1)= 3375 Σ^{<L}=  241\n",
      "     MAZE4X3: Σ=24 L=2  Σ^L=  576 Σ^(L+1)=13824 Σ^{<L}=  601\n",
      "      CHEESE: Σ=28 L=2  Σ^L=  784 Σ^(L+1)=21952 Σ^{<L}=  813\n"
     ]
    }
   ],
   "source": [
    "# Print some prorties (alphabet size, word length, resulting matrix sizes)\n",
    "def print_benchmark_properties():\n",
    "    for oomName in BenchData.OOMs + BenchData.IOOOMs:\n",
    "        bd = BenchData(oomName)\n",
    "        Σₒ, Σᵢ = bd.nO(), max(1, bd.nU())\n",
    "        Σ = Σₒ * Σᵢ\n",
    "        L = int(np.log(1200)/np.log(Σ))\n",
    "        print('%12s: Σ=%2d L=%d  Σ^L=%5d Σ^(L+1)=%5d Σ^{<L}=%5d'% (oomName, Σ, L, Σ**L, Σ**(L+1), sum([Σ**l for l in range(L+1)])))\n",
    "print_benchmark_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":...........:...........:..........."
     ]
    }
   ],
   "source": [
    "# Compute and save the `tom.Data` objects of the training data for RANDOM27_32 and length 10**8 (~0:20h and 3.2GB on disk)\n",
    "def pre_compute_RANDOM27_32_long_Data():\n",
    "    bd = BenchData('RANDOM27_32')\n",
    "    RANDOM27_32_max_data = [{}, {}, {}]\n",
    "    for ID in range(3):\n",
    "        print(':', end='', flush=True)\n",
    "        data = tom.Data()\n",
    "        data.sequence = bd.getSequence(ID)\n",
    "        wordSettings = [(0, 0, 1, maxWords) for maxWords in [32, 64, 128, 256, 512, 1024]]\n",
    "        wordSettings += [(0, 0, 'o_min', 1024), (1, 1, 1, 0), (2, 2, 1, 0), (0, 1, 1, 0), (0, 2, 1, 0)]\n",
    "        for wordSetting in wordSettings:\n",
    "            print('.', end='', flush=True)\n",
    "            wS = list(wordSetting)\n",
    "            if wS[2] == 'o_min': wS[2] = bd.o_min(10**8)\n",
    "            data.regularization = (2,3)\n",
    "            data.X = wS\n",
    "            data.Y = data.X\n",
    "            data.pre_compute()\n",
    "            data.V_YX(regularization=(0,0))\n",
    "            data.V_YX(regularization=(2,0))\n",
    "            with open(research_directory + '/results/RANDOM27_32_max_data/' + str(ID) + str(wS) + '.p', 'wb') as f:\n",
    "                pickle.dump({'cache': data._cache, 'X': data.X, 'Y': data.Y}, f)\n",
    "pre_compute_RANDOM27_32_long_Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%# Compute word setting results (~1:15h)\n",
    "%%px\n",
    "def compute_word_setting_results(oomName):\n",
    "    bd = BenchData(oomName)\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\"), str(oomName), file = f, flush=True)\n",
    "    Σₒ, Σᵢ = bd.nO(), max(1, bd.nU())\n",
    "    Σ = Σₒ * Σᵢ\n",
    "    wordSettings = [(0, 0, 1, maxWords) for maxWords in [32, 64, 128, 256, 512, 1024]]\n",
    "    wordSettings += [(0, 0, 'o_min', 1024)]\n",
    "    wordSettings += [(L, L, 1, 0) for L in range(1, int(np.log(1200)/np.log(Σ))+1)]\n",
    "    wordSettings += [(0, L, 1, 0) for L in range(1, int(np.log(1200)/np.log(Σ))+1)]\n",
    "    res[oomName] = {wS: {bd.dim(): {'GLS' : {regularization : []},\n",
    "                                    'SPEC': {None: []}} } for wS in wordSettings}\n",
    "    train = bd.getSequence(ID)\n",
    "    data = tom.Data()\n",
    "    data.sequence = train.sub(0)\n",
    "    data.regularization = regularization\n",
    "    for tl in bd.trainLengths():\n",
    "        print(time.strftime(\"    %Y-%m-%d %H:%M:%S\"), \"%8d \" % tl, file = f, end='', flush=True)\n",
    "        if tl < 10**8:\n",
    "            data.sequence = train.sub(tl)\n",
    "        for wordSetting in wordSettings:\n",
    "            print(\".\", file = f, end='', flush=True)\n",
    "            wS = list(wordSetting)\n",
    "            if wS[2] == 'o_min': wS[2] = bd.o_min(tl)\n",
    "            if tl < 10**8:\n",
    "                data.X = wS\n",
    "                data.Y = data.X\n",
    "                data.pre_compute()\n",
    "            else:\n",
    "                data._stree = None # This ensures that we rely only on the cache!\n",
    "                with open(research_directory + '/results/RANDOM27_32_max_data/' + str(ID) + str(wS) + '.p', 'rb') as f2:\n",
    "                    data_save = pickle.load(f2)\n",
    "                data.X = data_save['X']\n",
    "                data.Y = data_save['Y']\n",
    "                data._cache = data_save['cache']\n",
    "            spec = tom.learn.model_estimate(data, bd.dim(), method='SPEC')\n",
    "            res[oomName][wordSetting][bd.dim()]['SPEC'][None].append(bd.evaluate(spec))            \n",
    "            gls = tom.learn.model_estimate(data, bd.dim(), method='GLS')\n",
    "            res[oomName][wordSetting][bd.dim()]['GLS'][regularization].append(bd.evaluate(gls))\n",
    "            data.cache = []\n",
    "        print(\"done!\", file = f, flush=True)\n",
    "\n",
    "res = {}       \n",
    "with open(research_directory + '/results/px_log' + str(ID), 'w') as f:\n",
    "    for oomName in BenchData.OOMs + BenchData.IOOOMs: compute_word_setting_results(oomName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save word setting results to 'resultsWords.p'\n",
    "res = dview['res']\n",
    "with open(research_directory + '/results/resultsWords.p', 'wb') as f:\n",
    "    pickle.dump({'res':res, 'tom_version': tom.version}, f)\n",
    "del res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%# Compute dimension setting results (~2:35h)\n",
    "%%px\n",
    "def compute_dimension_setting_results(oomName):\n",
    "    bd = BenchData(oomName)\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\"), str(oomName), file = f, flush=True)\n",
    "    Σₒ, Σᵢ = bd.nO(), max(1, bd.nU())\n",
    "    Σ = Σₒ * Σᵢ\n",
    "    lenX = int(np.log(1200)/np.log(Σ))\n",
    "    wordSettings = [(0, 0, 'o_min', 1024), (0, lenX, 1, 0)]\n",
    "    if bd.dim() < 12:\n",
    "        dimSettings = list(range(1, bd.dim() + 1)) + [bd.dim() + 1, bd.dim() + 3, bd.dim() + 10]\n",
    "    else:\n",
    "        dimSettings =  [1,2,4,8,12,16,20,23,26,28,30,31,32,33,35,42]\n",
    "    res[oomName] = {wS: {d: {'GLS' : {regularization : []},\n",
    "                             'SPEC': {None: []}} for d in dimSettings} for wS in wordSettings}\n",
    "    train = bd.getSequence(ID)\n",
    "    data = tom.Data()\n",
    "    data.sequence = train.sub(0)\n",
    "    data.regularization = regularization\n",
    "    for tl in bd.trainLengths():\n",
    "        print(time.strftime(\"    %Y-%m-%d %H:%M:%S\"), \"%8d \" % tl, file = f, end='', flush=True)\n",
    "        if tl < 10**8:\n",
    "            data.sequence = train.sub(tl)\n",
    "        for wordSetting in wordSettings:\n",
    "            print(\":\", file = f, end='', flush=True)\n",
    "            wS = list(wordSetting)\n",
    "            if wS[2] == 'o_min': wS[2] = bd.o_min(tl)\n",
    "            if tl < 10**8:\n",
    "                data.X = wS\n",
    "                data.Y = data.X\n",
    "                data.pre_compute()\n",
    "            else:\n",
    "                data._stree = None # This ensures that we rely only on the cache!\n",
    "                with open(research_directory + '/results/RANDOM27_32_max_data/' + str(ID) + str(wS) + '.p', 'rb') as f2:\n",
    "                    data_save = pickle.load(f2)\n",
    "                data.X = data_save['X']\n",
    "                data.Y = data_save['Y']\n",
    "                data._cache = data_save['cache']\n",
    "            for dim in dimSettings:\n",
    "                spec = tom.learn.model_estimate(data, dim, method='SPEC')\n",
    "                res[oomName][wordSetting][dim]['SPEC'][None].append(bd.evaluate(spec))            \n",
    "                gls = tom.learn.model_estimate(data, dim, method='GLS')\n",
    "                res[oomName][wordSetting][dim]['GLS'][regularization].append(bd.evaluate(gls))\n",
    "                \n",
    "            data.cache = []\n",
    "        print(\"done!\", file = f, flush=True)\n",
    "\n",
    "res = {}\n",
    "with open(research_directory + '/results/px_log' + str(ID), 'w') as f:\n",
    "    for oomName in BenchData.OOMs + BenchData.IOOOMs: compute_dimension_setting_results(oomName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save dimension setting results to 'resultsDim.p'\n",
    "res = dview['res']\n",
    "with open(research_directory + '/results/resultsDim.p', 'wb') as f:\n",
    "    pickle.dump({'res':res, 'tom_version': tom.version}, f)\n",
    "del res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%# Compute dimension estimates (~0:20h)\n",
    "%%px\n",
    "def compute_dimension_estimates(oomName, wordSettings = [(0, 0, 'o_min', 1024), (0, 'lenX', 1, 0)], pqrs = [(0,0), (1, 1, (2,0))]):\n",
    "    bd = BenchData(oomName)\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\"), str(oomName), file = f, flush=True)\n",
    "    Σₒ, Σᵢ = bd.nO(), max(1, bd.nU())\n",
    "    Σ = Σₒ * Σᵢ\n",
    "    lenX = int(np.log(1200)/np.log(Σ))\n",
    "    res[oomName] = {tl: {wS: {pqr: {} for pqr in pqrs} for wS in wordSettings} for tl in bd.trainLengths()}\n",
    "    train = bd.getSequence(ID)\n",
    "    data = tom.Data()\n",
    "    data.sequence = train.sub(0)\n",
    "    for tl in bd.trainLengths():\n",
    "        print(time.strftime(\"    %Y-%m-%d %H:%M:%S\"), \"%8d \" % tl, file = f, end='', flush=True)\n",
    "        data.sequence = train.sub(tl)\n",
    "        for wordSetting in wordSettings:\n",
    "            print(\":\", file = f, end='', flush=True)\n",
    "            wS = list(wordSetting)\n",
    "            if wS[0] == 'lenX': wS[0] = lenX\n",
    "            if wS[1] == 'lenX': wS[1] = lenX                \n",
    "            if wS[2] == 'o_min': wS[2] = bd.o_min(tl)\n",
    "            data.X = wS\n",
    "            data.Y = data.X\n",
    "            data.regularization = (0,0)\n",
    "            data.pre_compute(only_F_and_V=True)\n",
    "            res[oomName][tl][wordSetting]['shape'] = (len(data.Y), len(data.X))\n",
    "            F = data.F_YX()\n",
    "            for pqr in pqrs:\n",
    "                v_Y, v_X = tom.learn.v_Y_v_X_from_data(data, *pqr)\n",
    "                U, s, VT = tom.linalg.cached_wsvd(v_Y**-0.5, F, v_X**-0.5)\n",
    "                res[oomName][tl][wordSetting][pqr]['spectrum'] = s\n",
    "                res_for_norm = res[oomName][tl][wordSetting][pqr]\n",
    "                print(\".\", file = f, end='', flush=True)\n",
    "                for norm in ['frob', 'spec', 'exspec', 'relative', 'avspec']:\n",
    "                    res_for_norm[norm] = tom.learn.numerical_rank(F, data.V_YX(), v_Y, v_X, norm, True)\n",
    "                mid_spec = (res_for_norm['avspec'][1] * res_for_norm['exspec'][1])**0.5\n",
    "                mid_spec_dim = 0\n",
    "                while mid_spec_dim < len(s) and s[mid_spec_dim] > mid_spec: mid_spec_dim += 1\n",
    "                res_for_norm['mid_spec'] = (mid_spec_dim, mid_spec)\n",
    "                frob = res_for_norm['frob']\n",
    "                res_for_norm['frob_mid_spec'] = (max(mid_spec_dim, frob[0]), min(mid_spec, frob[1]))\n",
    "            data.cache = []\n",
    "        print(\"done!\", file = f, flush=True)\n",
    "\n",
    "# Dimension estimation uses sampling to estimate the expectation of a spectral norm.\n",
    "# For reproducibility, we therefore seed the numpy random number generator. It is only used for this purpose.\n",
    "np.random.seed(123456789 + ID)\n",
    "res = {}\n",
    "with open(research_directory + '/results/px_log' + str(ID), 'w') as f:\n",
    "    for oomName in BenchData.OOMs + BenchData.IOOOMs: compute_dimension_estimates(oomName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save dimension estimates to 'resultsDimEstimation.p'\n",
    "res = dview['res']\n",
    "with open(research_directory + '/results/resultsDimEstimation.p', 'wb') as f:\n",
    "    pickle.dump({'res':res, 'tom_version': tom.version}, f)\n",
    "del res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%# Algorithm variant comparison incl. appropriate dimension estimates\n",
    "%%px --local\n",
    "def compute_variant_comparison(oomName, wordSetting = (0, 0, 'o_min', 1024)):\n",
    "    bd = BenchData(oomName)\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\"), str(oomName), file = f, flush=True)\n",
    "    Σₒ, Σᵢ = bd.nO(), max(1, bd.nU())\n",
    "    Σ = Σₒ * Σᵢ\n",
    "    \n",
    "    res[oomName] = {wordSetting: {'dim': [], 0: {algo: {'eval': []} for algo in ['SPEC', 'RCW', 'ES', 'WLS', 'GLS'] },\n",
    "                                  bd.dim(): {algo: {'eval': []} for algo in ['SPEC', 'RCW', 'ES', 'WLS', 'GLS'] }}}\n",
    "    train = bd.getSequence(ID)\n",
    "    data = tom.Data()\n",
    "    data.regularization = regularization\n",
    "    data.sequence = train.sub(0)\n",
    "    for tl in bd.trainLengths():\n",
    "        print(time.strftime(\"    %Y-%m-%d %H:%M:%S\"), \"%8d \" % tl, file = f, end='', flush=True)\n",
    "        if tl < 10**8:\n",
    "            data.sequence = train.sub(tl)\n",
    "        print(\".\", file = f, end='', flush=True)\n",
    "        wS = list(wordSetting)\n",
    "        if wS[2] == 'o_min': wS[2] = bd.o_min(tl)\n",
    "        if tl < 10**8:\n",
    "            data.X = wS\n",
    "            data.Y = data.X\n",
    "            data.regularization = regularization\n",
    "            data.pre_compute()\n",
    "        else:\n",
    "            data._stree = None # This ensures that we rely only on the cache!\n",
    "            with open(research_directory + '/results/RANDOM27_32_max_data/' + str(ID) + str(wS) + '.p', 'rb') as f2:\n",
    "                data_save = pickle.load(f2)\n",
    "            data.X = data_save['X']\n",
    "            data.Y = data_save['Y']\n",
    "            data._cache = data_save['cache']\n",
    "        v_Y, v_X = tom.learn.v_Y_v_X_from_data(data)\n",
    "        for d in set([0, bd.dim()]):\n",
    "            print(\":\", file = f, end='', flush=True)\n",
    "            if d != 0:\n",
    "                dim = bd.dim()\n",
    "            else:\n",
    "                dim = tom.learn.numerical_rank(data.F_YX(), data.V_YX(regularization=(0,0)), v_Y, v_X)\n",
    "                res[oomName][wordSetting]['dim'].append(dim)\n",
    "            print(\".\", file = f, end='', flush=True)\n",
    "\n",
    "            spec, subspace = tom.learn.model_estimate(data, dim, method='SPEC', return_subspace=True)\n",
    "            res[oomName][wordSetting][d]['SPEC']['eval'].append(bd.evaluate(spec))\n",
    "            print(\".\", file = f, end='', flush=True)\n",
    "\n",
    "            es = tom.learn.model_estimate(data, spec, v = (v_Y, v_X), method='ES')\n",
    "            res[oomName][wordSetting][d]['ES']['eval'].append(bd.evaluate(es))\n",
    "            print(\".\", file = f, end='', flush=True)\n",
    "\n",
    "            rcw = tom.learn.model_estimate(data, dim, method='RCW', v=(v_Y, v_X))\n",
    "            res[oomName][wordSetting][d]['RCW']['eval'].append(bd.evaluate(rcw))\n",
    "            print(\".\", file = f, end='', flush=True)\n",
    "\n",
    "            wls, subspace = tom.learn.model_estimate(data, subspace, method='WLS', return_subspace=True)\n",
    "            res[oomName][wordSetting][d]['WLS']['eval'].append(bd.evaluate(wls))\n",
    "            print(\".\", file = f, end='', flush=True)\n",
    "\n",
    "            if dim > 192:\n",
    "                subspace = tom.learn.subspace_by_alternating_projections(data.F_YX(), 192, data.V_YX(), ls_method='iCholesky')\n",
    "            gls = tom.learn.model_by_weighted_equations(data, subspace, ls_method='iCholesky')\n",
    "            res[oomName][wordSetting][d]['GLS']['eval'].append(bd.evaluate(gls))\n",
    "            print(\".\", file = f, end='', flush=True)\n",
    "        data.cache = []\n",
    "        print(\"done!\", file = f, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%# Compute dimension estimates and comparison results for synthetic benchmarks (~0:40h)\n",
    "%%px\n",
    "# Dimension estimation uses sampling to estimate the expectation of a spectral norm.\n",
    "# For reproducibility, we therefore seed the numpy random number generator. It is only used for this purpose.\n",
    "np.random.seed(123456789 + ID)\n",
    "res = {}\n",
    "with open(research_directory + '/results/px_log' + str(ID), 'w') as f:\n",
    "    for oomName in BenchData.OOMs + BenchData.IOOOMs: compute_variant_comparison(oomName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save comparison results to 'resultsSynthetic.p'\n",
    "res = dview['res']\n",
    "with open(research_directory + '/results/resultsSynthetic.p', 'wb') as f:\n",
    "    pickle.dump({'res':res, 'tom_version': tom.version}, f)\n",
    "del res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute and save dimension estimates and comparison results for real-world data (~ 7 h)\n",
    "\n",
    "# Dimension estimation uses sampling to estimate the expectation of a spectral norm.\n",
    "# For reproducibility, we therefore seed the numpy random number generator. It is only used for this purpose.\n",
    "np.random.seed(123456789)\n",
    "res = {}\n",
    "with open(research_directory + '/results/px_log' + str(ID), 'w') as f:\n",
    "    for oomName in BenchData.realWorldData: compute_variant_comparison(oomName)\n",
    "with open(research_directory + '/results/resultsRealworld.p', 'wb') as f:\n",
    "    pickle.dump({'res':res, 'tom_version': tom.version}, f)\n",
    "del res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning with missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0,
     2,
     12,
     17,
     26,
     38
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%# Settings for learning with missing values\n",
    "%%px --local\n",
    "class MissingRelevance(tom.stree.PositionRelevance):\n",
    "    def __init__(self, missingFactor = 0, notMissingFactor = 1):\n",
    "        self.missingFactor = missingFactor\n",
    "        self.notMissingFactor = notMissingFactor\n",
    "        super().__init__()\n",
    "    def compute(self, position):\n",
    "        seq = position.sequence()\n",
    "        nMissing = seq.inputSum()\n",
    "        nNotMissing = seq.length() - nMissing\n",
    "        return super().compute(position) * self.missingFactor**nMissing * self.notMissingFactor**nNotMissing\n",
    "def o_min(l, Σₒ, Σᵢ):\n",
    "    Σᵢ = max(1, Σᵢ)\n",
    "    Σ = Σₒ * Σᵢ\n",
    "    l = np.log(l) / (np.log(Σ) + np.log(Σᵢ))\n",
    "    return (l+1) * Σᵢ**l\n",
    "def get_dimensions_for_missing_benchmarks():\n",
    "    dims = {}\n",
    "    for oomName in BenchData.OOMs:\n",
    "        bd = BenchData(oomName)\n",
    "        dims[oomName] = len(bd.trainLengths()) * [bd.dim()]\n",
    "    with open(research_directory + '/results/resultsRealworld.p', 'rb') as f:\n",
    "        res = pickle.load(f)['res']\n",
    "    for oomName in BenchData.realWorldData: dims[oomName] = res[oomName][(0, 0, 'o_min', 1024)]['dim']\n",
    "    return dims\n",
    "def cx_validate_missing(loom, cx_test, stabilization=(0.0002, 0.03, 5, 1e-8)):\n",
    "    moom = tom.Oom(loom.dimension(), loom.nOutputSymbols() + 1, 2)\n",
    "    moom.sig(loom.sig())\n",
    "    for o in range(loom.nOutputSymbols()):\n",
    "        moom.tau(o, 0, loom.tau(o))\n",
    "        moom.tau(o, 1, np.zeros((loom.dimension(), loom.dimension())))\n",
    "    moom.tau(loom.nOutputSymbols(), 0, np.zeros((loom.dimension(), loom.dimension())))\n",
    "    moom.tau(loom.nOutputSymbols(), 1, sum([loom.tau(o) for o in range(loom.nOutputSymbols())]))\n",
    "    moom.w0(loom.w0())\n",
    "    moom.initialize()\n",
    "    moom.stabilization(*stabilization)\n",
    "    return moom.l2l(cx_test)*cx_test.length()/(cx_test.length()-cx_test.inputSum())\n",
    "def compute_missing_value_results(oomName, dims, regular=False, stabilization=(0.0002, 0.03, 5, 1e-8)):\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\"), str(oomName), file = f, flush=True)\n",
    "    bd = BenchData(oomName)\n",
    "    Σₒ = bd.nO()\n",
    "    res[oomName] = {mP: {wordSettingAndWildcard: {reg: {'SPEC': {'cx' : [], 'eval' : []}, 'Weighted' : {'cx' : [], 'eval' : []}}\n",
    "                                                  for reg in regularizations}\n",
    "                         for wordSettingAndWildcard in wordSettingsAndWildcards} for mP in missingProbs}\n",
    "    for mP in missingProbs:\n",
    "        print(time.strftime(\"   %Y-%m-%d %H:%M:%S\"), str(oomName), str(mP), file = f, flush=True)\n",
    "        data = tom.Data()\n",
    "        train = bd.getSequence(ID, missingProb=mP, regular=regular, nU=1)\n",
    "        data.sequence = train.sub(0)\n",
    "        for tli, tl in enumerate(bd.trainLengths()):\n",
    "            print(time.strftime(\"      %Y-%m-%d %H:%M:%S\"), \"%8d \" % tl, file = f, end='', flush=True)\n",
    "            data.sequence = train.sub(tl)\n",
    "            data.nInputSymbols = 1\n",
    "            cx_test = train.sub(tl)[-int(2e5):]\n",
    "            for wordSetting, wildcard in wordSettingsAndWildcards:\n",
    "                wS = []\n",
    "                for i in wordSetting:\n",
    "                    wS.append(eval(i) if type(i) is str else i)\n",
    "                data.X = wS\n",
    "                data.Y = data.X\n",
    "                data._estimator = tom.EstimatorMCAR(data.stree) if wildcard else tom.Estimator(data.stree)\n",
    "                for regularization in regularizations:\n",
    "                    reg = []\n",
    "                    for i in regularization: # Note: Cannot use list comprehension due to `eval` here!\n",
    "                        reg.append(eval(i) if type(i) is str else i)\n",
    "                    data.regularization = reg\n",
    "                    data.pre_compute()\n",
    "                    print(\".\", file = f, end='', flush=True)\n",
    "                    dim = dims[oomName][tli]\n",
    "                    spec, subspace = tom.learn.model_estimate(data, dim, method='SPEC', return_subspace=True)\n",
    "                    spec.setIO(False)\n",
    "                    res[oomName][mP][(wordSetting, wildcard)][regularization]['SPEC']['eval'].append(bd.evaluate(spec, stabilization=stabilization))\n",
    "                    res[oomName][mP][(wordSetting, wildcard)][regularization]['SPEC']['cx'].append(cx_validate_missing(spec, cx_test, stabilization))\n",
    "                    print(\".\", file = f, end='', flush=True)\n",
    "\n",
    "                    weighted = tom.learn.model_estimate(data, subspace, method='WLS' if dim > 192 else 'GLS', ls_method='iCholesky')\n",
    "                    weighted.setIO(False)\n",
    "                    res[oomName][mP][(wordSetting, wildcard)][regularization]['Weighted']['eval'].append(bd.evaluate(weighted, stabilization=stabilization))\n",
    "                    res[oomName][mP][(wordSetting, wildcard)][regularization]['Weighted']['cx'].append(cx_validate_missing(weighted, cx_test, stabilization))                        \n",
    "                    print(\".\", file = f, end='', flush=True)\n",
    "\n",
    "                    del spec, subspace, weighted\n",
    "                    data.cache = []\n",
    "                del wS\n",
    "            print(\"done!\", file = f, flush=True)\n",
    "\n",
    "regularizations = [(2,3), (2,'3/tl**2')]\n",
    "missingProbs = [1/9.5, 1/4.5, 1/2.5]\n",
    "wordSettingsAndWildcards = [((0, 0, 'o_min(tl,Σₒ+1,1)', 1024, False, False, 'MissingRelevance((Σₒ)**-1,1)'), False),\n",
    "                            ((0, 0, 'o_min(tl,Σₒ+1,1)', 1024, False, False, 'MissingRelevance((Σₒ)**-1,1)'), True),\n",
    "                            ((0, 0, 'o_min(tl,Σₒ+1,1)', 1024, False, False, 'MissingRelevance(0,1)'), False)]\n",
    "dims_for_missing = get_dimensions_for_missing_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute and save results with missing values for OOM benchmarks (~10:00 h)\n",
    "%px res = {}\n",
    "%px with open(research_directory + '/results/px_log' + str(ID), 'w') as f: [compute_missing_value_results(oomName, dims_for_missing, regular=False) for oomName in BenchData.OOMs]\n",
    "res = dview['res']\n",
    "with open(research_directory + '/results/resultsMissingRandom.p', 'wb') as f: pickle.dump({'res':res, 'tom_version': tom.version}, f)\n",
    "del res\n",
    "%px res = {}\n",
    "%px with open(research_directory + '/results/px_log' + str(ID), 'w') as f: [compute_missing_value_results(oomName, dims_for_missing, regular=True) for oomName in BenchData.OOMs]\n",
    "res = dview['res']\n",
    "with open(research_directory + '/results/resultsMissingRegular.p', 'wb') as f: pickle.dump({'res':res, 'tom_version': tom.version}, f)\n",
    "del res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute and save results with random missing values for real-world benchmarks (~19 h)\n",
    "res = {}\n",
    "with open(research_directory + '/results/px_log' + str(ID), 'w') as f: [compute_missing_value_results(oomName, dims_for_missing, regular=False) for oomName in BenchData.realWorldData]\n",
    "with open(research_directory + '/results/resultsMissingRandomRW.p', 'wb') as f: pickle.dump({'res':res, 'tom_version': tom.version}, f)\n",
    "del res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute and save results with regular missing values for real-world benchmarks (~17 h)\n",
    "res = {}\n",
    "with open(research_directory + '/results/px_log' + str(ID), 'w') as f: [compute_missing_value_results(oomName, dims_for_missing, regular=True) for oomName in BenchData.realWorldData]\n",
    "with open(research_directory + '/results/resultsMissingRegularRW.p', 'wb') as f: pickle.dump({'res':res, 'tom_version': tom.version}, f)\n",
    "del res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute and save results with regular missing values for BIBLE27 benchmark with tweaked dimension (~6h)\n",
    "missingProbs = [1/4.5]\n",
    "wordSettingsAndWildcards = [((0, 0, 'o_min(tl,Σₒ+1,1)', 1024, False, False, 'MissingRelevance((Σₒ)**-1,1)'), True)]\n",
    "res = {}\n",
    "with open(research_directory + '/results/px_log' + str(ID), 'w') as f:\n",
    "    compute_missing_value_results('BIBLE27', {'BIBLE27': [8, 16, 32, 64, 96, 128, 160, 192]}, regular=True, stabilization=(0.0002, 0.01, 5, 1e-8))\n",
    "with open(research_directory + '/results/resultsMissingRegularBIBLE27tweakedDim.p', 'wb') as f:\n",
    "    pickle.dump({'res':res, 'tom_version': tom.version}, f)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
